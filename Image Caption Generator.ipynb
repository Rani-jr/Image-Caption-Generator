{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "807336d7",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5379e6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from os import listdir\n",
    "from pickle import dump, load\n",
    "\n",
    "import PIL.Image\n",
    "from IPython.display import Image,display\n",
    "\n",
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input , Dense , LSTM , Embedding , Dropout\n",
    "from tensorflow.keras.layers import add\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef20569",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80113371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading a text file\n",
    "\n",
    "def load_text(fname):\n",
    "    f = open(fname, 'r')\n",
    "    text = f.read()\n",
    "    f.close\n",
    "    return text\n",
    "\n",
    "# Using the previous function, we will create a function to get images&captions\n",
    "\n",
    "def images_with_captions(fname):\n",
    "    ourtext = load_text(fname)\n",
    "    captions = ourtext.split('\\n')\n",
    "    img_cap = {}\n",
    "    for line in captions[:-1]:\n",
    "        image, caption = line.split('\\t')\n",
    "        if image[:-2] not in img_cap:\n",
    "            img_cap[image[:-2]] = [caption]\n",
    "        else:\n",
    "            img_cap[image[:-2]].append(caption)\n",
    "    return img_cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1fa2e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning: Cleaning captions\n",
    "\n",
    "def clean_data(image_caption):\n",
    "    table = str.maketrans(\"\", \"\", string.punctuation)\n",
    "    for image, caption in image_caption.items():\n",
    "        for i, cap in enumerate(caption):\n",
    "            \n",
    "            s = cap.replace(\"-\", \" \")\n",
    "            \n",
    "            # removing punctuation\n",
    "            s = s.translate(table)\n",
    "            \n",
    "            s = s.split()\n",
    "            \n",
    "            # Lower casing\n",
    "            s = [tok.lower() for tok in s]\n",
    "            # Removing \" A \" and \" 's \"\n",
    "            s = [tok for tok in s if(len(tok)>1)]\n",
    "            # Remove tokens with numbers\n",
    "            s = [tok for tok in s if(tok.isalpha())]\n",
    "            # Getting back our string\n",
    "            cap = \" \".join(s)\n",
    "            \n",
    "            image_caption[image][i] = cap\n",
    "    \n",
    "    return image_caption    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71406b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting all unique words\n",
    "\n",
    "def vocab(image_caption):\n",
    "    v = set()\n",
    "    for k in image_caption.keys():\n",
    "        [v.update(d.split()) for d in image_caption[k]]\n",
    "    \n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "762bee52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating our image-caption file\n",
    "\n",
    "def img_cap_file(image_caption, fname):\n",
    "    img_cap = list()\n",
    "    for k, caps in image_caption.items():\n",
    "        for cap in caps:\n",
    "            img_cap.append(k + \"\\t\" + cap)\n",
    "    \n",
    "    data = \"\\n\".join(img_cap)\n",
    "    \n",
    "    # Create the file\n",
    "    f = open(fname, 'w')\n",
    "    f.write(data)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f622e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_dir = \"C:/ImageCaptionGenerator/Flickr8k_text\"\n",
    "image_dir = \"C:/ImageCaptionGenerator/Flickr8k_Dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e34cb83e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of images_caps:  8092\n",
      "Vocabulary size:  8422\n"
     ]
    }
   ],
   "source": [
    "# File of raw captions\n",
    "fname = text_dir + \"/\" + \"Flickr8k.token.txt\"\n",
    "# Mapping each image to its captions\n",
    "image_caps = images_with_captions(fname)\n",
    "print(\"Size of images_caps: \", len(image_caps))\n",
    "# Cleaning captions\n",
    "cleaned = clean_data(image_caps)\n",
    "#Building our vocab\n",
    "vocabulary = vocab(cleaned)\n",
    "print(\"Vocabulary size: \", len(vocabulary))\n",
    "#Creating our image_captions file\n",
    "img_cap_file(cleaned, \"C:/ImageCaptionGenerator/image_with_caption.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80bea981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a function to extract features\n",
    "from tensorflow.keras.applications.xception import Xception\n",
    "from tensorflow.keras.applications.xception import preprocess_input\n",
    "\n",
    "def extract_features(path):\n",
    "    #We will use Xception pretrained model\n",
    "    model = Xception(include_top=False, pooling='avg')\n",
    "    features = {}\n",
    "    \n",
    "    for image in os.listdir(path):\n",
    "        fname = path + \"/\" + image\n",
    "        img = PIL.Image.open(fname)\n",
    "        img = img.resize((299,299))\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "        img = preprocess_input(img)\n",
    "        \n",
    "        feature = model.predict(img)\n",
    "        features[image] = feature\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8884e16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = extract_features(image_dir)\n",
    "dump(features, open(\"C:/ImageCaptionGenerator/features.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ceaf583e",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = load(open(\"C:/ImageCaptionGenerator/features.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4acd60fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "\n",
    "def get_images(fname):\n",
    "    f = load_text(fname)\n",
    "    images = f.split(\"\\n\")[:-1]\n",
    "    return images\n",
    "\n",
    "def get_clean_captions(fname, images):\n",
    "    f = load_text(fname)\n",
    "    captions = {}\n",
    "    for cap in f.split(\"\\n\"):\n",
    "        \n",
    "        tokens = cap.split()\n",
    "        if len(tokens)<1:\n",
    "            continue\n",
    "        \n",
    "        img, caption = tokens[0], tokens[1:]\n",
    "        \n",
    "        if img in images:\n",
    "            if img not in captions:\n",
    "                captions[img] = []\n",
    "            desc = '<start> ' + \" \".join(caption) + ' <end>'\n",
    "            captions[img].append(desc)\n",
    "    \n",
    "    return captions\n",
    "\n",
    "def load_features(images):\n",
    "    #loading all features from 'features.p'\n",
    "    all_features = load(open(\"C:/ImageCaptionGenerator/features.p\", \"rb\"))\n",
    "    #Selecting needed features\n",
    "    needed_features = {k:all_features[k] for k in images}\n",
    "    return needed_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67499230",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = text_dir + \"/\" + \"Flickr_8k.trainImages.txt\"\n",
    "imgs_caps_path = \"C:/ImageCaptionGenerator/image_with_caption.txt\"\n",
    "\n",
    "training_images = get_images(fname)\n",
    "training_captions = get_clean_captions(imgs_caps_path, training_images)\n",
    "training_features = load_features(training_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce7204c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list containing all clean captions\n",
    "def captions_in_list(captions):\n",
    "    all_caps = []\n",
    "    for k in captions.keys():\n",
    "        [all_caps.append(cap) for cap in captions[k]]\n",
    "    \n",
    "    return all_caps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ee16f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a tokenizer to give each word an index\n",
    "\n",
    "def tokenize(captions):\n",
    "    caps_list = captions_in_list(captions)\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(caps_list)\n",
    "    \n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2c6f32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a 'tokenizer.p' file\n",
    "tokenizer = tokenize(training_captions)\n",
    "dump(tokenizer, open('C:/ImageCaptionGenerator/tokenizer.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b146237a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  7318\n"
     ]
    }
   ],
   "source": [
    "# Number of words, vocabulary size\n",
    "vocabulary_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary size: ', vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f8ad68c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating the length of the longest caption\n",
    "\n",
    "def max_length(captions):\n",
    "    caps_list = captions_in_list(captions)\n",
    "    longest = max(len(d.split()) for d in caps_list)\n",
    "    \n",
    "    return longest\n",
    "\n",
    "max_length = max_length(image_caps)\n",
    "max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f9b218",
   "metadata": {},
   "source": [
    "# Building The Model\n",
    "\n",
    "Define our model based on the merge-model in these papers:\n",
    "\n",
    "* https://arxiv.org/abs/1703.09137\n",
    "* https://arxiv.org/abs/1708.02043\n",
    "\n",
    "for more details you can see this post:\n",
    "\n",
    "* https://machinelearningmastery.com/caption-generation-inject-merge-architectures-encoder-decoder-model/\n",
    "\n",
    "Our model contains three parts:\n",
    "\n",
    "1 Photo feature extractor: We preprocessed the images with the Xception model and we will use the extracted features predicted by this model \n",
    "\n",
    "2 Sequence processor: A word embedding layer that handles text, followed by LSTM layer \n",
    "\n",
    "3 Decoder: The feature extractor(1) and the sequence processor(2) produce fixed-length vector. They are merged together and processed by a Dense layer to make a final prediction\n",
    "\n",
    "We will use Dropout as a regularization technique to reduce overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a0a219cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(captions, features, tokenizer, max_length):\n",
    "    while 1:\n",
    "        for k, caps_list in captions.items():\n",
    "            #get photo features\n",
    "            feature = features[k][0]\n",
    "            input_image, input_sequence, output_word = create_sequences(tokenizer, max_length, caps_list, feature)\n",
    "            yield [input_image, input_sequence], output_word     \n",
    "\n",
    "def create_sequences(tokenizer, max_length, desc_list, feature):\n",
    "    X1, X2, y = list(), list(), list()\n",
    "    # walk through each description for the image\n",
    "    for desc in desc_list:\n",
    "        # encode the sequence\n",
    "        seq = tokenizer.texts_to_sequences([desc])[0]\n",
    "        # split one sequence into multiple X,y pairs\n",
    "        for i in range(1, len(seq)):\n",
    "            # split into input and output pair\n",
    "            in_seq, out_seq = seq[:i], seq[i]\n",
    "            # pad input sequence\n",
    "            in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "            # encode output sequence\n",
    "            out_seq = to_categorical([out_seq], num_classes=vocabulary_size)[0]\n",
    "            # store\n",
    "            X1.append(feature)\n",
    "            X2.append(in_seq)\n",
    "            y.append(out_seq)\n",
    "    return np.array(X1), np.array(X2), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4b7c0ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the model\n",
    "\n",
    "def create_model(vocabulary_size, max_length):\n",
    "    \n",
    "    input1 = Input(shape=(2048,))\n",
    "    feat1 = Dropout(0.5)(input1)\n",
    "    feat2 = Dense(256, activation='relu')(feat1)\n",
    "    \n",
    "    input2 = Input(shape=(max_length,))\n",
    "    seq1 = Embedding(vocabulary_size, 256, mask_zero=True)(input2)\n",
    "    seq2 = Dropout(0.5)(seq1)\n",
    "    seq3 = LSTM(256)(seq2)\n",
    "    \n",
    "    decoder1 = add([feat2, seq3])\n",
    "    decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "    output = Dense(vocabulary_size, activation='softmax')(decoder2)\n",
    "    \n",
    "    model = Model(inputs=[input1, input2], outputs=output)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    \n",
    "    print(model.summary())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2a5185",
   "metadata": {},
   "source": [
    "Training The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c1926de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset:  6000\n",
      "Training Captions:  6000\n",
      "Vocabulary Size: 7318\n",
      "Description Length:  33\n"
     ]
    }
   ],
   "source": [
    "print('Training Dataset: ', len(training_images))\n",
    "print('Training Captions: ', len(training_captions))\n",
    "print('Vocabulary Size:', vocabulary_size)\n",
    "print('Description Length: ', max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7697d143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 33)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            [(None, 2048)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 33, 256)      1873408     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 2048)         0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 33, 256)      0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          524544      dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 256)          525312      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 256)          0           dense[0][0]                      \n",
      "                                                                 lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          65792       add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 7318)         1880726     dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 4,869,782\n",
      "Trainable params: 4,869,782\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "6000/6000 [==============================] - 1782s 296ms/step - loss: 4.5019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SI RANY\\anaconda3\\envs\\JREnv\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\generic_utils.py:494: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  warnings.warn('Custom mask layers require a config and must override '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000/6000 [==============================] - 1831s 305ms/step - loss: 3.6521\n",
      "6000/6000 [==============================] - 1720s 287ms/step - loss: 3.3699\n",
      "6000/6000 [==============================] - 1721s 287ms/step - loss: 3.1980\n",
      "6000/6000 [==============================] - 1721s 287ms/step - loss: 3.0825\n",
      "6000/6000 [==============================] - 1811s 302ms/step - loss: 2.9942\n",
      "6000/6000 [==============================] - 1948s 325ms/step - loss: 2.9284\n",
      "6000/6000 [==============================] - 1885s 314ms/step - loss: 2.8753\n",
      "6000/6000 [==============================] - 1841s 307ms/step - loss: 2.8293\n",
      "6000/6000 [==============================] - 1755s 292ms/step - loss: 2.7937\n"
     ]
    }
   ],
   "source": [
    "model = create_model(vocabulary_size, max_length)\n",
    "epochs = 10\n",
    "steps = len(training_captions)\n",
    "\n",
    "for i in range(epochs):\n",
    "    generator = data_generator(training_captions, training_features, tokenizer, max_length)\n",
    "    model.fit(generator, epochs=1, steps_per_epoch=steps)\n",
    "    model.save(\"C:/ImageCaptionGenerator/model_\" + str(i+1) + \".h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32871776",
   "metadata": {},
   "source": [
    "#### To test the model, run the \"GenerateCaption.py\" file in the terminal as follow:\n",
    "\n",
    "## python GenerateCaption.py -i \"image path\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
